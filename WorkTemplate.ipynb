{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375aacc5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Técnicas Matemáticas para Big Data - Project NN?\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c68c",
   "metadata": {},
   "source": [
    "GROUP NN:\n",
    "- Rodrigo Pinto - Nº 103280 - ??% Work Participation\n",
    "- Inês Silva - Nº 134870 - ??% Work Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fb045",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 1. Introduction to the problem of study [1,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7827eaa",
   "metadata": {},
   "source": [
    "GitHub is a collaborative development platform that promotes collaboration between developers. This platform allows multiple people to work simultaneously on the same project, using commands such as pull requests, commits, and others. Analyzing these activities allows us to understand collaboration patterns, measure developer activity, and more.\n",
    "\n",
    "One of the main challenges in this analysis is determining the number of unique users interacting with repositories in a given period of time, such as each month. Accurately counting these users requires the storage and processing of huge volumes of data, which leads to high memory costs.\n",
    "\n",
    "A possible solution to this problem is the use of the HyperLogLog (HLL) algorithm. This algorithm allows for the efficient estimation of the number of unique elements in large datasets.\n",
    "\n",
    "In this project, the objective is to apply the Hyperloglog algorithm to estimate the number of unique users interacting daily with a GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af2342",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Brief and general description of the approach and methods used [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e324615",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Brief History and literature review of the problem and methods/algorithms [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889a980",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 4. About the main method/algorithm used [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52795c65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Python imports and global configurations [0,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f3deb",
   "metadata": {},
   "source": [
    "### Install and import the necessary libraries to compute the Bayesian Network and perform other methods  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5177dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasketch in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (1.6.5)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from datasketch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from datasketch) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pybloom-live in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: bitarray>=0.3.4 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pybloom-live) (3.7.2)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from pybloom-live) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: hyperloglog in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: msgpack in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from hyperloglog) (1.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\miniconda3\\envs\\fcd\\lib\\site-packages (from hyperloglog) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pomegranate\n",
    "# %pip install torch\n",
    "# %pip install Pillow\n",
    "%pip install duckdb pandas requests tqdm\n",
    "%pip install datasketch\n",
    "%pip install pybloom-live\n",
    "%pip install hyperloglog\n",
    "import time\n",
    "import math, hashlib\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from pybloom_live import BloomFilter\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import hyperloglog\n",
    "import redis\n",
    "import random\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb739aae",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 6. Dataset and variables explanation [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c91d1",
   "metadata": {},
   "source": [
    "### 6.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando 2024-10-01:  29%|██▉       | 7/24 [01:19<03:11, 11.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_date \u001b[38;5;241m<\u001b[39m END_DATE:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hour \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m24\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m         records \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m         all_records\u001b[38;5;241m.\u001b[39mextend(records)\n\u001b[0;32m     74\u001b[0m     current_date \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m, in \u001b[0;36mdownload_and_parse\u001b[1;34m(date_obj, hour)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         e \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m         records\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: e\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor_login\u001b[39m\u001b[38;5;124m\"\u001b[39m: e\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m: e\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m         })\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\envs\\FCD\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\envs\\FCD\\Lib\\json\\decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m--> 338\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# CONFIGURAÇÕES\n",
    "# ----------------------------------------------\n",
    "# Pasta local onde os ficheiros .json.gz serão guardados\n",
    "DATA_DIR = \"gharchive_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Base de dados DuckDB\n",
    "DB_PATH = \"github_archive.duckdb\"\n",
    "\n",
    "# Intervalo de tempo (exemplo: 3 dias)\n",
    "START_DATE = datetime(2024, 10, 1)\n",
    "END_DATE = datetime(2024, 10, 2)  # até, mas não incluindo este dia\n",
    "\n",
    "# ----------------------------------------------\n",
    "# FUNÇÃO PARA DOWNLOAD E LEITURA DE UM FICHEIRO\n",
    "# ----------------------------------------------\n",
    "def download_and_parse(date_obj, hour):\n",
    "    \"\"\"Faz download de um ficheiro GHArchive para uma data/hora específica.\"\"\"\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    url = f\"https://data.gharchive.org/{date_str}-{hour}.json.gz\"\n",
    "    local_path = os.path.join(DATA_DIR, f\"{date_str}-{hour}.json.gz\")\n",
    "\n",
    "    # Se já existir localmente, não faz download novamente\n",
    "    if not os.path.exists(local_path):\n",
    "        r = requests.get(url, timeout=60)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        else:\n",
    "            print(f\"[Aviso] Falhou o download de {url} (status {r.status_code})\")\n",
    "            return []\n",
    "\n",
    "    # Lê o conteúdo\n",
    "    with gzip.open(local_path, \"rb\") as f:\n",
    "        data = f.read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "    # Extrai apenas os campos relevantes\n",
    "    records = []\n",
    "    for line in data:\n",
    "        try:\n",
    "            e = json.loads(line)\n",
    "            records.append({\n",
    "                \"actor_id\": e.get(\"actor\", {}).get(\"id\"),\n",
    "                \"actor_login\": e.get(\"actor\", {}).get(\"login\"),\n",
    "                \"repo_name\": e.get(\"repo\", {}).get(\"name\"),\n",
    "                \"type\": e.get(\"type\"),\n",
    "                \"created_at\": e.get(\"created_at\")\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return records\n",
    "\n",
    "# ----------------------------------------------\n",
    "# LOOP PRINCIPAL DE DOWNLOAD + ARMAZENAMENTO\n",
    "# ----------------------------------------------\n",
    "all_records = []\n",
    "\n",
    "current_date = START_DATE\n",
    "while current_date < END_DATE:\n",
    "    for hour in tqdm(range(24), desc=f\"Processando {current_date.date()}\"):\n",
    "        records = download_and_parse(current_date, hour)\n",
    "        all_records.extend(records)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Converte para DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "print(f\"\\nTotal de eventos recolhidos: {len(df):,}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# GRAVAÇÃO NA BASE DE DADOS DUCKDB\n",
    "# ----------------------------------------------\n",
    "con = duckdb.connect(DB_PATH)\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS github_events (\n",
    "    actor_id BIGINT,\n",
    "    actor_login VARCHAR,\n",
    "    repo_name VARCHAR,\n",
    "    type VARCHAR,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.register(\"tmp_df\", df)\n",
    "con.execute(\"INSERT INTO github_events SELECT * FROM tmp_df\")\n",
    "con.close()\n",
    "\n",
    "print(f\"\\n✅ Dados guardados com sucesso em {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0657f0",
   "metadata": {},
   "source": [
    "### 6.2 Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0656ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A carregar ficheiros locais: 100%|██████████| 48/48 [10:45<00:00, 13.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de eventos extraídos: 10,174,100\n",
      "✅ Inseridos 10,174,100 novos registos na base github_archive.duckdb\n"
     ]
    }
   ],
   "source": [
    "# Caminhos\n",
    "DATA_DIR = \"gharchive_data\"\n",
    "DB_PATH = \"github_archive.duckdb\"\n",
    "\n",
    "# Função para ler ficheiro local\n",
    "def read_local_file(path):\n",
    "    \"\"\"Lê ficheiro JSON.GZ local e devolve lista de registos relevantes.\"\"\"\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        data = f.read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "    records = []\n",
    "    for line in data:\n",
    "        try:\n",
    "            e = json.loads(line)\n",
    "            records.append({\n",
    "                \"actor_id\": e.get(\"actor\", {}).get(\"id\"),\n",
    "                \"actor_login\": e.get(\"actor\", {}).get(\"login\"),\n",
    "                \"repo_name\": e.get(\"repo\", {}).get(\"name\"),\n",
    "                \"type\": e.get(\"type\"),\n",
    "                \"created_at\": e.get(\"created_at\")\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return records\n",
    "\n",
    "\n",
    "# Lê todos os ficheiros .json.gz da pasta\n",
    "files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith(\".json.gz\")])\n",
    "\n",
    "all_records = []\n",
    "for file in tqdm(files, desc=\"A carregar ficheiros locais\"):\n",
    "    path = os.path.join(DATA_DIR, file)\n",
    "    records = read_local_file(path)\n",
    "    all_records.extend(records)\n",
    "\n",
    "# Converte para DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "print(f\"\\nTotal de eventos extraídos: {len(df):,}\")\n",
    "\n",
    "# Conexão à base DuckDB\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "# Cria tabela se não existir\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS github_events (\n",
    "    actor_id BIGINT,\n",
    "    actor_login VARCHAR,\n",
    "    repo_name VARCHAR,\n",
    "    type VARCHAR,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Regista dataframe temporário e insere na tabela\n",
    "con.register(\"tmp_df\", df)\n",
    "con.execute(\"INSERT INTO github_events SELECT * FROM tmp_df\")\n",
    "con.close()\n",
    "\n",
    "print(f\"✅ Inseridos {len(df):,} novos registos na base {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059d654",
   "metadata": {},
   "source": [
    "### 6.3 Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f471263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Small view of the dataset: -------\n",
      "   actor_id          actor_login                    repo_name       type  \\\n",
      "0  74199667              Chroiud          Chroiud/playercount  PushEvent   \n",
      "1   5622661          utilForever              utilForever/BOJ  PushEvent   \n",
      "2  49699333      dependabot[bot]  mppanayotov/angular-capital  PushEvent   \n",
      "3  41898282  github-actions[bot]            ePlus-DEV/metrics  PushEvent   \n",
      "4  49556955        JonatasAlves9         JonatasAlves9/Doctor  PushEvent   \n",
      "\n",
      "  created_at  \n",
      "0 2024-10-01  \n",
      "1 2024-10-01  \n",
      "2 2024-10-01  \n",
      "3 2024-10-01  \n",
      "4 2024-10-01  \n",
      "\n",
      "------- Datatypes: ------- \n",
      "actor_id                int64\n",
      "actor_login            object\n",
      "repo_name              object\n",
      "type                   object\n",
      "created_at     datetime64[us]\n",
      "dtype: object\n",
      "\n",
      "------- Missing values: -------\n",
      "actor_id       0\n",
      "actor_login    0\n",
      "repo_name      0\n",
      "type           0\n",
      "created_at     0\n",
      "dtype: int64\n",
      "\n",
      "Total events read: 10,174,100\n",
      "\n",
      "Total users (w/ repetion): 1,123,357\n"
     ]
    }
   ],
   "source": [
    "# Ligação à base existente\n",
    "con = duckdb.connect(\"github_archive.duckdb\")\n",
    "\n",
    "# Leitura dos dados necessários\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT actor_id, actor_login, repo_name, type, created_at\n",
    "    FROM github_events\n",
    "    WHERE actor_login IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"------- Small view of the dataset: -------\\n{df.head()}\")\n",
    "print(f\"\\n------- Datatypes: ------- \\n{df.dtypes}\")\n",
    "print(f\"\\n------- Missing values: -------\\n{df.isnull().sum()}\")\n",
    "\n",
    "print(f\"\\nTotal events read: {len(df):,}\")\n",
    "print(f\"\\nTotal users (w/ repetion): {df['actor_login'].nunique():,}\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5703771",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 7. Main code as possible solution to the problem [1,5 valor] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c983df",
   "metadata": {},
   "source": [
    "### 7.1 HLL \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ce4d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimativa de utilizadores únicos (HLL): 1,511,083\n",
      "Valor Real de utilizadores únicos: 1,123,357\n",
      "Respectivo erro percentual: 34.51%\n",
      "Valor Real de utilizadores únicos (Query): 1,122,592\n",
      "Respectivo erro percentual: 34.61%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# FUNÇÕES HLL\n",
    "# -----------------------------------\n",
    "\n",
    "def hash_function(value):\n",
    "    hash_value = hashlib.sha256(str(value).encode('utf8')).hexdigest()\n",
    "    return int(hash_value, 16)\n",
    "\n",
    "def leftmost_1_bit_position(hash_value):\n",
    "    bin_hash = bin(hash_value)[2:]\n",
    "    for i, bit in enumerate(bin_hash):\n",
    "        if bit == '1':\n",
    "            return i + 1\n",
    "    return len(bin_hash) + 1\n",
    "\n",
    "def process_element(element, registers, m):\n",
    "    hash_value = hash_function(element)\n",
    "    p = int(math.log2(m))\n",
    "    register_index = hash_value & (m - 1)\n",
    "    remaining_hash = hash_value >> p\n",
    "    position = leftmost_1_bit_position(remaining_hash)\n",
    "    registers[register_index] = max(registers[register_index], position)\n",
    "\n",
    "def harmonic_mean(registers):\n",
    "    sum_of_inverses = sum([2**-reg for reg in registers])\n",
    "    return len(registers) / sum_of_inverses\n",
    "\n",
    "def bias_correction(registers, raw_estimate, m):\n",
    "    if raw_estimate <= 2.5 * m:\n",
    "        V = registers.count(0)\n",
    "        if V > 0:\n",
    "            return m * math.log(m / V)\n",
    "    elif raw_estimate > (2**32) / 30:\n",
    "        return -(2**32) * math.log(1 - raw_estimate / (2**32))\n",
    "    return raw_estimate\n",
    "\n",
    "def estimate_cardinality(registers):\n",
    "    m = len(registers)\n",
    "    alpha_m = 0.7213 / (1 + 1.079 / m)\n",
    "    raw_estimate = alpha_m * m**2 * harmonic_mean(registers)\n",
    "    return int(round(bias_correction(registers, raw_estimate, m),0))\n",
    "\n",
    "# -----------------------------------\n",
    "# FUNÇÃO PARA OBTER NÚMERO DE EVENTOS\n",
    "# -----------------------------------\n",
    "\n",
    "def eventos():\n",
    "    con = duckdb.connect(\"github_archive.duckdb\")\n",
    "    n_users_exato_query = con.execute(\"\"\"\n",
    "        SELECT COUNT(DISTINCT actor_id)\n",
    "        FROM github_events\n",
    "        WHERE actor_id IS NOT NULL\"\"\").fetchone()[0]\n",
    "    con.close()\n",
    "    return n_users_exato_query\n",
    "\n",
    "# -----------------------------------\n",
    "# APLICAR HYPERLOGLOG\n",
    "# -----------------------------------\n",
    "\n",
    "m = 1024  # número de registos, podes alterar\n",
    "registers = [0] * m\n",
    "\n",
    "for user in df['actor_login']:\n",
    "    process_element(user, registers, m)\n",
    "\n",
    "estimate = estimate_cardinality(registers)\n",
    "n_real_dataframe = df[\"actor_login\"].nunique() #eventos()  # número real de eventos\n",
    "n_real_query = eventos()  # número real de eventos\n",
    "erro_percentual = abs(100 * (estimate / n_real_query - 1))\n",
    "erro_abs = abs(estimate /n_real_dataframe - 1) * 100\n",
    "\n",
    "print(f\"Estimativa de utilizadores únicos (HLL): {estimate:,}\")\n",
    "print(f\"Valor Real de utilizadores únicos: {n_real_dataframe:,}\\nRespectivo erro percentual: {erro_abs:.2f}%\")\n",
    "print(f\"Valor Real de utilizadores únicos (Query): {n_real_query:,}\\nRespectivo erro percentual: {erro_percentual:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de09fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimativa de utilizadores únicos (HLL): 5,498,006\n",
      "Valor Real de utilizadores únicos: 1,123,357\n",
      "Respectivo erro percentual: 389.43%\n",
      "Valor Real de utilizadores únicos (Query): 1,122,592\n",
      "Respectivo erro percentual: 389.76%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# FUNÇÕES HLL\n",
    "# -----------------------------------\n",
    "\n",
    "def hash_function(value):\n",
    "    hash_value = hashlib.sha256(str(value).encode('utf8')).hexdigest()\n",
    "    return int(hash_value, 16)\n",
    "\n",
    "def leftmost_1_bit_position(hash_value):\n",
    "    bin_hash = bin(hash_value)[2:]\n",
    "    for i, bit in enumerate(bin_hash):\n",
    "        if bit == '1':\n",
    "            return i + 1\n",
    "    return len(bin_hash) + 1\n",
    "\n",
    "def process_element(element, registers, m):\n",
    "    hash_value = hash_function(element)\n",
    "    p = int(math.log2(m))\n",
    "    register_index = hash_value & (m - 1)\n",
    "    remaining_hash = hash_value >> p\n",
    "    position = leftmost_1_bit_position(remaining_hash)\n",
    "    registers[register_index] = max(registers[register_index], position)\n",
    "\n",
    "def harmonic_mean(registers):\n",
    "    sum_of_inverses = sum([2**-reg for reg in registers])\n",
    "    return len(registers) / sum_of_inverses\n",
    "\n",
    "def bias_correction(registers, raw_estimate, m):\n",
    "    if raw_estimate <= 2.5 * m:\n",
    "        V = registers.count(0)\n",
    "        if V > 0:\n",
    "            return m * math.log(m / V)\n",
    "    elif raw_estimate > (2**32) / 30:\n",
    "        return -(2**32) * math.log(1 - raw_estimate / (2**32))\n",
    "    return raw_estimate\n",
    "\n",
    "def estimate_cardinality(registers, a, b):\n",
    "    m = len(registers)\n",
    "    alpha_m = 0.7213 / (a + b / m)\n",
    "    raw_estimate = alpha_m * m**2 * harmonic_mean(registers)\n",
    "    return int(round(bias_correction(registers, raw_estimate, m),0))\n",
    "\n",
    "def hyperloglog_estimate(dataset, m, n, a, b):\n",
    "    registers = [0] * m\n",
    "    for element in dataset:\n",
    "        process_element(element, registers, m)\n",
    "    estimate =  int(round(estimate_cardinality(registers, a, b),0))\n",
    "    error = abs(100*(estimate/n - 1))\n",
    "    return estimate, error, registers\n",
    "\n",
    "# -----------------------------------\n",
    "# FUNÇÃO PARA OBTER NÚMERO DE EVENTOS\n",
    "# -----------------------------------\n",
    "\n",
    "def eventos():\n",
    "    con = duckdb.connect(\"github_archive.duckdb\")\n",
    "    n_users_exato_query = con.execute(\"\"\"\n",
    "        SELECT COUNT(DISTINCT actor_id)\n",
    "        FROM github_events\n",
    "        WHERE actor_id IS NOT NULL\"\"\").fetchone()[0]\n",
    "    con.close()\n",
    "    return n_users_exato_query\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# APLICAR HYPERLOGLOG\n",
    "# -----------------------------------\n",
    "\"\"\"dataset = df['actor_login'].tolist()\n",
    "n = df['actor_login'].nunique()\n",
    "pL = []; errorL = []\n",
    "min_p = None; min_estimate = None; min_error = 9999999\n",
    "for p in range(10,12):\n",
    "    m = 2**p     #Número de buckets\n",
    "    estimate = None; error = 9999999\n",
    "    for n_a in range(1,11): \n",
    "        for n_b in range(1,11):\n",
    "            a = 0.7213 * n_a/5; b = 1.079 * n_b/5\n",
    "            estimateT, errorT, registers = hyperloglog_estimate(dataset, m, n, a, b)\n",
    "            if (error>errorT):\n",
    "                error = errorT\n",
    "                estimate = estimateT\n",
    "    pL.append(p); errorL.append(error)\n",
    "    if (min_error>error):\n",
    "        min_p = p\n",
    "        min_error = error\n",
    "        min_estimate = estimate\n",
    "        \n",
    "print(f\"#dataset={len(dataset)} real_difNums={n} estimate_difNums={min_estimate} min(p)={min_p} min(error)={min_error:.2f}%\")\n",
    "sz1 = sys.getsizeof(registers)\n",
    "sz2 = sys.getsizeof(dataset)\n",
    "print(f\"#registers={2**min_p} size(registers)={sz1} size(dataset)={sz2} Perc={100*sz1/sz2:.2f}%\")\n",
    "plt.plot(pL,errorL)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "m = 2**11  # número de registos, podes alterar\n",
    "registers = [0] * m\n",
    "\n",
    "for user in df['actor_login']:\n",
    "    process_element(user, registers, m)\n",
    "\n",
    "estimate = estimate_cardinality(registers, 1.1, 1.079)\n",
    "n_real_dataframe = df[\"actor_login\"].nunique() #eventos()  # número real de eventos\n",
    "n_real_query = eventos()  # número real de eventos\n",
    "erro_percentual = abs(100 * (estimate / n_real_query - 1))\n",
    "erro_abs = abs(estimate /n_real_dataframe - 1) * 100\n",
    "\n",
    "print(f\"Estimativa de utilizadores únicos (HLL): {estimate:,}\")\n",
    "print(f\"Valor Real de utilizadores únicos: {n_real_dataframe:,}\\nRespectivo erro percentual: {erro_abs:.2f}%\")\n",
    "print(f\"Valor Real de utilizadores únicos (Query): {n_real_query:,}\\nRespectivo erro percentual: {erro_percentual:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d5476",
   "metadata": {},
   "source": [
    "- Ler artigo HLL google HyperLogLog in Practice: Algorithmic Engineering of a\n",
    "State of The Art Cardinality Estimation Algorithm e 2007\n",
    "- Ler Chabchoub et al., Sliding HyperLogLog for Port Scan Detection, 2014\n",
    "- Calcular valor optimo mudando os parametro m, a e b.\n",
    "- Comparar a memoria consumida, com a query sql\n",
    "- Comparar o consumo de memoria para o HLL by hand, HLL python, HLL redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8af3fc",
   "metadata": {},
   "source": [
    "### 7.2 HLL using a dedicated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8139c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de eventos lidos: 10,174,100\n",
      "\n",
      "Estimativa de utilizadores únicos (HLL): 1,139,943\n",
      "Número exato de utilizadores únicos contados através de Query: 1,122,592\n",
      "Número exato de utilizadores únicos contados através de DataFrame: 1,122,592\n",
      "Erro percentual do HLL vs valor exato (Query): 1.55%\n",
      "Erro percentual do HLL vs valor exato (DataFrame): 1.55%\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import hyperloglog\n",
    "\n",
    "print(f\"Total de eventos lidos: {len(df):,}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# APLICAR HYPERLOGLOG\n",
    "# --------------------------------------------------\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "for actor in df['actor_id']:\n",
    "    hll.add(actor)\n",
    "\n",
    "estimate = len(hll)\n",
    "print(f\"\\nEstimativa de utilizadores únicos (HLL): {estimate:,}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# VALOR EXATO PARA COMPARAÇÃO\n",
    "# --------------------------------------------------\n",
    "con = duckdb.connect(\"github_archive.duckdb\")\n",
    "n_users_exato_query = con.execute(\"\"\"\n",
    "    SELECT COUNT(DISTINCT actor_id)\n",
    "    FROM github_events\n",
    "    WHERE actor_id IS NOT NULL\n",
    "\"\"\").fetchone()[0]\n",
    "con.close()\n",
    "\n",
    "n_users_exato_query_len = df[\"actor_id\"].nunique()\n",
    "\n",
    "print(f\"Número exato de utilizadores únicos contados através de Query: {n_users_exato_query:,}\")\n",
    "print(f\"Número exato de utilizadores únicos contados através de DataFrame: {n_users_exato_query_len:,}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# ERRO PERCENTUAL\n",
    "# --------------------------------------------------\n",
    "erro_percentual_query = abs(100 * (estimate / n_users_exato_query - 1))\n",
    "erro_percentual_df = abs(100 * (estimate / n_users_exato_query_len - 1))\n",
    "\n",
    "print(f\"Erro percentual do HLL vs valor exato (Query): {erro_percentual_query:.2f}%\")\n",
    "print(f\"Erro percentual do HLL vs valor exato (DataFrame): {erro_percentual_df:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab78d9c",
   "metadata": {},
   "source": [
    "### 7.3 Redis HLL Container\n",
    "- Apply redis HLL\n",
    "- Look up to other HLL\n",
    "- Apply Bloom Filters to verify if a actor_id is in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conexão ao Redis (assumindo que corre em localhost:6379)\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "# Chave no Redis\n",
    "HLL_KEY = \"dataset:hll\"\n",
    "\n",
    "# Limpar antes de começar (caso já exista)\n",
    "r.delete(HLL_KEY)\n",
    "\n",
    "# Adicionar cada valor ao HyperLogLog\n",
    "for value in df['actor_id']:\n",
    "    r.pfadd(HLL_KEY, value)\n",
    "\n",
    "redis_estimate = r.pfcount(HLL_KEY)\n",
    "print(f\"Real distinto: {n_real}']}, Redis estima: {redis_estimate}\")\n",
    "error = abs(redis_estimate / n_real -1) * 100\n",
    "print(f\"Erro do Redis: {error:.2f}%\")\n",
    "print(f\"Python estimate = {estimate:.2f}\")\n",
    "print(f\"Redis HLL estimate = {redis_estimate}\")\n",
    "print(f\"Erro customizado = {erro_percentual_df:.2f}%\")\n",
    "print(f\"Erro Redis = {abs(redis_estimate - n_real) / n_real * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa86cc3",
   "metadata": {},
   "source": [
    "### 7.4 Results Visualization\n",
    "In this section you can analyse the results in a ilustrative way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15554",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 8. Analysis of Example 1 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7810c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 9. Analysis of Example 2 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821172",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 10. Pros and cons of the approach [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60974d68",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 11. Future improvements [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1eb04",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <br><br>\n",
    "    <p style=\"font-size: 40px;\">References [1,0 valor]</p>\n",
    "</div>\n",
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
